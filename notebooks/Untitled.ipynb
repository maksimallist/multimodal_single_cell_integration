{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736855d3-c2f8-4263-9326-63b877aee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514bd47-5eb9-4f9c-86e8-5995b1e682ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import h5py\n",
    "# не удалять! import hdf5plugin !\n",
    "import hdf5plugin\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b7ef31-ef17-4984-868b-6035c1eec255",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '/home/mks/PycharmProjects/multimodal_single_cell_integration/src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/mks/PycharmProjects/multimodal_single_cell_integration/src\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '/home/mks/PycharmProjects/multimodal_single_cell_integration/src'"
     ]
    }
   ],
   "source": [
    "class FSCCDataset(Dataset):\n",
    "    file_types = ['inputs', 'targets']\n",
    "    h5_reserved_names: List[str] = ['train_multi_inputs', 'train_multi_targets', 'train_cite_inputs',\n",
    "                                    'train_cite_targets', 'test_multi_inputs', 'test_cite_inputs']\n",
    "\n",
    "    dataflows = {'cite': {'train': {'inputs': None, 'targets': None},\n",
    "                          'test': {'inputs': None}},\n",
    "                 'multi': {'train': {'inputs': None, 'targets': None},\n",
    "                           'test': {'inputs': None}}}\n",
    "\n",
    "    metadata = None\n",
    "    meta_unique_vals: Dict = {}\n",
    "    metadata_file: str = 'metadata.csv'\n",
    "    meta_transform_names: List[str] = ['day', 'donor', 'cell_type']\n",
    "    meta_names: List[str] = ['day', 'donor', 'cell_type', 'technology']\n",
    "    meta_keys: List[str] = ['cell_id', 'day', 'donor', 'cell_type', 'technology']\n",
    "\n",
    "    col_name: str = 'axis0'\n",
    "    pos_name: str = 'position'\n",
    "    index_name: str = 'cell_id'\n",
    "    cell_id_name: str = \"axis1\"\n",
    "    target_name: str = 'gene_id'\n",
    "    features_name: str = \"block0_values\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_path: Union[str, Path],\n",
    "                 task: str, mode: str,\n",
    "                 meta_transform: Optional[str] = None,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None):\n",
    "        self.task = task\n",
    "        self.mode = mode\n",
    "        self.data_ids = None\n",
    "        self.data_shapes = None\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.meta_transform = meta_transform\n",
    "        # init dataset\n",
    "        self._read_task_dataset(dataset_path)\n",
    "\n",
    "    def _read_metadata(self, path: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path, index_col=self.index_name)\n",
    "        for key in self.meta_names:\n",
    "            self.meta_unique_vals[key] = list(df[key].unique())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _transform_metalabels(self, meta_dict: Dict, cell_id: str) -> Dict:\n",
    "        if self.meta_transform:\n",
    "            if self.meta_transform == 'index':\n",
    "                for key in self.meta_transform_names:\n",
    "                    meta_dict[key] = self.meta_unique_vals[key].index(self.metadata[key][cell_id])\n",
    "            elif self.meta_transform == 'one_hot':\n",
    "                for key in self.meta_transform_names:\n",
    "                    one_hot_vector = np.zeros((len(self.meta_unique_vals[key]),))\n",
    "                    one_hot_vector[self.meta_unique_vals[key].index(self.metadata[key][cell_id])] = 1\n",
    "                    meta_dict[key] = one_hot_vector\n",
    "            else:\n",
    "                raise ValueError(f\"The argument 'meta_transform' can only take values from a list \"\n",
    "                                 f\"['index', 'one_hot', None], but '{self.meta_transform}' was found.\")\n",
    "        else:\n",
    "            meta_dict = {key: self.metadata[key][cell_id] for key in self.meta_names}\n",
    "\n",
    "        return meta_dict\n",
    "\n",
    "    def _get_task_flow(self, folder_path: Path, mode: str, task: str, file_type: str) -> None:\n",
    "        file_name = '_'.join([mode, task, file_type])\n",
    "        print(f\"[ Reading {file_name}.h5 file ... ]\")\n",
    "        f_path = str(folder_path.joinpath(f\"{file_name}.h5\").absolute())\n",
    "        flow, feature_shape = self.get_hdf5_flow(f_path)\n",
    "        # write data in structure\n",
    "        self.dataflows[task][mode][file_type] = flow\n",
    "        self.data_shapes[task][mode][file_type] = feature_shape\n",
    "        print(f\"[ Reading {file_name}.h5 file is complete. ]\")\n",
    "\n",
    "    def _read_task_dataset(self, folder_path: Union[str, Path]) -> None:\n",
    "        self.data_shapes = {self.task: {self.mode: {s: None for s in self.file_types}}}\n",
    "\n",
    "        if isinstance(folder_path, str):\n",
    "            folder_path = Path(folder_path)\n",
    "        # read metadata file\n",
    "        self.metadata = self._read_metadata(str(folder_path.joinpath(self.metadata_file)))\n",
    "        # read all h5 files\n",
    "        if self.mode == 'train':\n",
    "            for file_type in self.file_types:\n",
    "                self._get_task_flow(folder_path, self.mode, self.task, file_type)\n",
    "        elif self.mode == 'test':\n",
    "            self._get_task_flow(folder_path, self.mode, self.task, self.file_types[0])\n",
    "        else:\n",
    "            raise ValueError(f\"Argument 'mode' can only take values from a list: ['train', 'test'], \"\n",
    "                             f\"but {self.mode} was found.\")\n",
    "\n",
    "        self.data_ids = self._set_data_ids()\n",
    "\n",
    "    def _set_data_ids(self):\n",
    "        feature_flow = self.dataflows[self.task][self.mode]['inputs']\n",
    "        return [x.decode(\"utf-8\") for x in feature_flow[self.cell_id_name]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_ids)\n",
    "\n",
    "    def __getitem__(self, item: int) -> Dict:\n",
    "        cell_id = self.data_ids[item]\n",
    "        features = self.dataflows[self.task][self.mode]['inputs']\n",
    "        meta_data = {self.index_name: cell_id, self.pos_name: features[self.col_name][item].decode(\"utf-8\")}\n",
    "        meta_data = self._transform_metalabels(meta_data, cell_id)\n",
    "\n",
    "        x = features[self.features_name][item]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        meta_data[self.file_types[0]] = x\n",
    "\n",
    "        if self.dataflows[self.task][self.mode].get('targets'):\n",
    "            targets = self.dataflows[self.task][self.mode]['targets']\n",
    "            meta_data[self.target_name] = targets[self.col_name][item].decode(\"utf-8\")\n",
    "            y = targets[self.features_name][item]\n",
    "\n",
    "            if self.target_transform:\n",
    "                y = self.target_transform(y)\n",
    "\n",
    "            meta_data[self.file_types[1]] = y\n",
    "\n",
    "            return meta_data\n",
    "        else:\n",
    "            return meta_data\n",
    "\n",
    "    def get_hdf5_flow(self, file_path: str):\n",
    "        file_flow = h5py.File(file_path, 'r')\n",
    "\n",
    "        file_keys = list(file_flow.keys())\n",
    "        assert len(file_keys) == 1, AssertionError(f\"Incorrect file format, '{file_path}' file have more than one \"\n",
    "                                                   f\"group: {file_keys}.\")\n",
    "\n",
    "        file_name = file_keys[0]\n",
    "        assert file_name in self.h5_reserved_names, \\\n",
    "            AssertionError(f\"Incorrect file format, group name must be in {self.h5_reserved_names}, \"\n",
    "                           f\"but {file_name} was found.\")\n",
    "\n",
    "        datasets_names = list(file_flow[file_name])\n",
    "        assert self.features_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name \"\n",
    "                                                                    f\"{self.features_name} was not found in hdf5 file \"\n",
    "                                                                    f\"datasets list.\")\n",
    "        assert self.cell_id_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name \"\n",
    "                                                                   f\"{self.cell_id_name} was not found in hdf5 file \"\n",
    "                                                                   f\"datasets list.\")\n",
    "        assert self.col_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name {self.col_name} \"\n",
    "                                                               f\"was not found in hdf5 file datasets list.\")\n",
    "\n",
    "        lines, features_shape = file_flow[file_name][self.features_name].shape\n",
    "\n",
    "        return file_flow[file_name], (lines, features_shape)\n",
    "\n",
    "    def reindex_dataset(self,\n",
    "                        day: Optional[Union[int, List[int]]] = None,\n",
    "                        donor: Optional[Union[int, List[int]]] = None,\n",
    "                        cell_type: Optional[Union[str, List[str]]] = None) -> None:\n",
    "        conditions = []\n",
    "        if (day is not None) and isinstance(day, int):\n",
    "            conditions.append((self.metadata['day'] == day))\n",
    "        elif (day is not None) and isinstance(day, list):\n",
    "            conditions.append((self.metadata['day'].isin(day)))\n",
    "\n",
    "        if (donor is not None) and isinstance(donor, int):\n",
    "            conditions.append((self.metadata['donor'] == donor))\n",
    "        elif (donor is not None) and isinstance(donor, list):\n",
    "            conditions.append((self.metadata['donor'].isin(donor)))\n",
    "\n",
    "        if (cell_type is not None) and isinstance(cell_type, int):\n",
    "            conditions.append((self.metadata['cell_type'] == cell_type))\n",
    "        elif (cell_type is not None) and isinstance(cell_type, list):\n",
    "            conditions.append((self.metadata['cell_type'].isin(cell_type)))\n",
    "\n",
    "        if len(conditions) > 0:\n",
    "            feature_flow = self.dataflows[self.task][self.mode]['inputs']\n",
    "            ids = {x.decode(\"utf-8\") for x in feature_flow[self.cell_id_name]}\n",
    "\n",
    "            final_cond = conditions[0]\n",
    "            if len(conditions) > 1:\n",
    "                for cond in conditions[1:]:\n",
    "                    final_cond &= cond\n",
    "\n",
    "            cond_index = set(self.metadata[final_cond].index)\n",
    "            self.data_ids = list(cond_index & ids)\n",
    "\n",
    "    def rebase(self, task: Optional[str] = None, mode: Optional[str] = None):\n",
    "        if task is not None:\n",
    "            self.task = task\n",
    "        if mode is not None:\n",
    "            self.mode = mode\n",
    "\n",
    "        self._read_task_dataset(self.dataset_path)\n",
    "        self.data_ids = self._set_data_ids()\n",
    "\n",
    "    def set_length(self, length: int) -> None:\n",
    "        self.data_ids = self.data_ids[:length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3cd82-b747-4c3c-8569-40ba6addb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..src.dataset import FSCCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a02e8b-f220-4290-a414-e774d79a09ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35293ebf-92cd-498b-897a-28f5ef5875d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0172de52-b34b-4c4d-a19d-59ae77b9de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine dataset folder and get column names\n",
    "dataset_path = Path('/home/mks/PycharmProjects/multimodal_single_cell_integration/dataset')\n",
    "submissions_path = Path('/home/mks/PycharmProjects/multimodal_single_cell_integration/submissions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e98223-42ec-48fb-bb27-497708157d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Read evaluation_ids file ... ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[ Read evaluation_ids file ... ]\")\n",
    "evaluation_path = str(dataset_path.joinpath('evaluation_ids.csv'))\n",
    "col_list = ['row_id', 'cell_id']  # , 'cell_id'\n",
    "evaluation_ids = pd.read_csv(evaluation_path, usecols=col_list, index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92c2e8f-1d8a-4c30-ac1e-5663c13c18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cells = evaluation_ids['cell_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66624163-c83c-43aa-8fcf-18af79cd8b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65443,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cells.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2580f4c-273d-4ef1-aa7d-dc3ba6fc471d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a08cb7cc-befa-4ad7-9230-8b859033d3d5",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8719658d-af06-4d7e-a999-b018c7dd5a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Read cite predictions ... ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[ Read cite predictions ... ]\")\n",
    "cite_sub = np.load(str(submissions_path.joinpath('cite', 'my', 'conv_mse_corr.npy')), mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92c78941-5894-4af5-a9b2-aa68faeda0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48203, 140)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1fb5594-db6c-4080-ab61-18b3ff17ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_sub = np.around(cite_sub, decimals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbdec2b4-97a0-4d89-8212-0cc9e2803d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.000000e+00, -0.000000e+00, -0.000000e+00,  6.435830e+00,\n",
       "        4.721320e+00,  7.896390e+00,  8.945870e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -2.100000e-04, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00,  1.209729e+01,  2.172560e+00,\n",
       "        4.989360e+00,  2.740000e-02,  1.211140e+00, -0.000000e+00,\n",
       "       -0.000000e+00,  2.200190e+00, -0.000000e+00, -0.000000e+00,\n",
       "        8.656100e+00, -0.000000e+00, -0.000000e+00, -6.000000e-05,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00,  1.041316e+01, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00,  1.002073e+01,\n",
       "       -9.300000e-04, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "        5.004540e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "        4.028800e-01, -0.000000e+00,  1.536020e+00,  1.006640e+00,\n",
       "       -0.000000e+00,  3.480780e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "        2.701600e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00,  4.222420e+00, -0.000000e+00,  8.404900e+00,\n",
       "       -0.000000e+00,  2.062660e+00, -0.000000e+00, -0.000000e+00,\n",
       "        1.613560e+00, -3.300000e-04, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "        2.042920e+00, -1.309000e-02, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00,  1.103780e+00, -0.000000e+00,\n",
       "       -0.000000e+00,  8.847420e+00, -0.000000e+00,  2.637950e+00,\n",
       "        4.695590e+00, -0.000000e+00,  1.895930e+00, -0.000000e+00,\n",
       "        5.597660e+00, -0.000000e+00,  5.373930e+00, -0.000000e+00,\n",
       "        1.077369e+01,  3.016310e+00,  1.243910e+00,  3.803900e+00,\n",
       "       -0.000000e+00, -8.792000e-02, -0.000000e+00,  2.321200e-01,\n",
       "        2.317060e+00, -0.000000e+00, -0.000000e+00,  1.275727e+01,\n",
       "       -0.000000e+00,  1.522380e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00,  9.762830e+00,\n",
       "       -0.000000e+00, -0.000000e+00, -0.000000e+00, -0.000000e+00,\n",
       "        7.716950e+00, -0.000000e+00,  3.239100e+00,  2.817110e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f413246-e4b4-4541-bbe4-6ed0b8945554",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_sub = cite_sub.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaba7be6-9499-47ea-93c9-eee308adf3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6748420,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed3c79a-e6fb-4dfe-ac07-c57e24b41804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Read cite predictions ... ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[ Read cite predictions ... ]\")\n",
    "cite_sub_2 = np.load(str(submissions_path.joinpath('cite', 'my', 'old_true.npy')), mmap_mode='r')\n",
    "# with open(str(submissions_path.joinpath('cite', 'kaggle', 'tune-lgbm-only-final.pickle')), 'rb') as f:\n",
    "#     cite_sub = pickle.load(f)\n",
    "cite_sub_2 = cite_sub_2.flatten()\n",
    "# evaluation_ids['target'] = pd.Series(cite_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fb33915-1163-4f0a-a431-d178b4c52c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6748420,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a31ad5a8-f8fc-4bb2-aaa2-58ab078c6a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6812820,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3607900-e10f-412b-852b-47e0fe212fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6812820 - 6748420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b01989c9-fe56-48e4-a15b-587399fa849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0 == cite_sub[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34184052-93d6-4a17-9eb2-645c8b65cbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63281f1-056a-4df2-bd8c-67bc1d1d020c",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d2335c-629a-4b68-84e1-821a116b9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Read mutiome predictions ... ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[ Read mutiome predictions ... ]\")\n",
    "multiome_sub = str(submissions_path.joinpath('multiome', 'ura',\n",
    "                                             'mmsc_svd_gena_features_gene_atac_gene_id_all_29.09.csv'))\n",
    "multiome_sub = pd.read_csv(multiome_sub, index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07eeffd4-aa03-41b4-9e75-cf991e4bb4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65744180, 1)\n"
     ]
    }
   ],
   "source": [
    "print(multiome_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b274ad0-f871-4eb0-b662-ae2c7a195bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58929920, 1)\n"
     ]
    }
   ],
   "source": [
    "multiome_sub = multiome_sub.loc[~(multiome_sub==0).all(axis=1)]\n",
    "print(multiome_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c043a5e-2a01-49dd-beb6-b0d946471cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Read mutiome predictions ... ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[ Read mutiome predictions ... ]\")\n",
    "multiome_2 = str(submissions_path.joinpath('multiome', 'ura', 'submission_all_targets.csv'))\n",
    "multiome_2 = pd.read_csv(multiome_2, index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57b0ea0-e3e6-44a1-9ba9-e23af5a4f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65744180, 1)\n"
     ]
    }
   ],
   "source": [
    "print(multiome_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74fcb116-d064-4730-9cf7-74a8909299b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58916730, 1)\n"
     ]
    }
   ],
   "source": [
    "multiome_2 = multiome_2.loc[~(multiome_2==0).all(axis=1)]\n",
    "print(multiome_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dba5f47d-c23e-449a-885e-1e83c4cdbb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13190"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58916730 - 58929920"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb1dcf-c5dc-47c1-979c-5c6e72aed4f7",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8d292-7e12-4214-902d-92660b13bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[ Make submission ... ]\")\n",
    "del evaluation_ids['row_id']\n",
    "evaluation_ids = evaluation_ids.dropna()\n",
    "evaluation_ids = evaluation_ids.reset_index(drop=True)\n",
    "assert not evaluation_ids['target'].isna().any()\n",
    "save_path = str(submissions_path.joinpath('both',\n",
    "                                          'conv_mse_corr-mmsc_svd_gena_features_gene_atac_gene_id_all_29.09.csv'))\n",
    "evaluation_ids.to_csv(save_path, index_label='row_id')\n",
    "print(f\"[ Make submission is done. ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efa5bc-0574-4f0e-9bec-9b068909540e",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
