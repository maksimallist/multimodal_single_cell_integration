{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736855d3-c2f8-4263-9326-63b877aee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import math\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "# не удалять! import hdf5plugin !\n",
    "import hdf5plugin\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b7ef31-ef17-4984-868b-6035c1eec255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSCCDataset(Dataset):\n",
    "    file_types = ['inputs', 'targets']\n",
    "    h5_reserved_names: List[str] = ['train_multi_inputs', 'train_multi_targets', 'train_cite_inputs',\n",
    "                                    'train_cite_targets', 'test_multi_inputs', 'test_cite_inputs']\n",
    "\n",
    "    dataflows = {'cite': {'train': {'inputs': None, 'targets': None},\n",
    "                          'test': {'inputs': None}},\n",
    "                 'multi': {'train': {'inputs': None, 'targets': None},\n",
    "                           'test': {'inputs': None}}}\n",
    "\n",
    "    metadata = None\n",
    "    meta_unique_vals: Dict = {}\n",
    "    metadata_file: str = 'metadata.csv'\n",
    "    meta_transform_names: List[str] = ['day', 'donor', 'cell_type']\n",
    "    meta_names: List[str] = ['day', 'donor', 'cell_type', 'technology']\n",
    "    meta_keys: List[str] = ['cell_id', 'day', 'donor', 'cell_type', 'technology']\n",
    "\n",
    "    col_name: str = 'axis0'\n",
    "    pos_name: str = 'position'\n",
    "    index_name: str = 'cell_id'\n",
    "    cell_id_name: str = \"axis1\"\n",
    "    target_name: str = 'gene_id'\n",
    "    features_name: str = \"block0_values\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_path: Union[str, Path],\n",
    "                 task: str, mode: str,\n",
    "                 meta_transform: Optional[str] = None,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None):\n",
    "        self.task = task\n",
    "        self.mode = mode\n",
    "        self.data_ids = None\n",
    "        self.data_shapes = None\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.meta_transform = meta_transform\n",
    "        # init dataset\n",
    "        self._read_task_dataset(dataset_path)\n",
    "\n",
    "    def _read_metadata(self, path: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path, index_col=self.index_name)\n",
    "        for key in self.meta_names:\n",
    "            self.meta_unique_vals[key] = list(df[key].unique())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _transform_metalabels(self, meta_dict: Dict, cell_id: str) -> Dict:\n",
    "        if self.meta_transform:\n",
    "            if self.meta_transform == 'index':\n",
    "                for key in self.meta_transform_names:\n",
    "                    meta_dict[key] = self.meta_unique_vals[key].index(self.metadata[key][cell_id])\n",
    "            elif self.meta_transform == 'one_hot':\n",
    "                for key in self.meta_transform_names:\n",
    "                    one_hot_vector = np.zeros((len(self.meta_unique_vals[key]),))\n",
    "                    one_hot_vector[self.meta_unique_vals[key].index(self.metadata[key][cell_id])] = 1\n",
    "                    meta_dict[key] = one_hot_vector\n",
    "            else:\n",
    "                raise ValueError(f\"The argument 'meta_transform' can only take values from a list \"\n",
    "                                 f\"['index', 'one_hot', None], but '{self.meta_transform}' was found.\")\n",
    "        else:\n",
    "            meta_dict = {key: self.metadata[key][cell_id] for key in self.meta_names}\n",
    "\n",
    "        return meta_dict\n",
    "\n",
    "    def _get_task_flow(self, folder_path: Path, mode: str, task: str, file_type: str) -> None:\n",
    "        file_name = '_'.join([mode, task, file_type])\n",
    "        print(f\"[ Reading {file_name}.h5 file ... ]\")\n",
    "        f_path = str(folder_path.joinpath(f\"{file_name}.h5\").absolute())\n",
    "        flow, feature_shape = self.get_hdf5_flow(f_path)\n",
    "        # write data in structure\n",
    "        self.dataflows[task][mode][file_type] = flow\n",
    "        self.data_shapes[task][mode][file_type] = feature_shape\n",
    "        print(f\"[ Reading {file_name}.h5 file is complete. ]\")\n",
    "\n",
    "    def _read_task_dataset(self, folder_path: Union[str, Path]) -> None:\n",
    "        self.data_shapes = {self.task: {self.mode: {s: None for s in self.file_types}}}\n",
    "\n",
    "        if isinstance(folder_path, str):\n",
    "            folder_path = Path(folder_path)\n",
    "        # read metadata file\n",
    "        self.metadata = self._read_metadata(str(folder_path.joinpath(self.metadata_file)))\n",
    "        # read all h5 files\n",
    "        if self.mode == 'train':\n",
    "            for file_type in self.file_types:\n",
    "                self._get_task_flow(folder_path, self.mode, self.task, file_type)\n",
    "        elif self.mode == 'test':\n",
    "            self._get_task_flow(folder_path, self.mode, self.task, self.file_types[0])\n",
    "        else:\n",
    "            raise ValueError(f\"Argument 'mode' can only take values from a list: ['train', 'test'], \"\n",
    "                             f\"but {self.mode} was found.\")\n",
    "\n",
    "        self.data_ids = self._set_data_ids()\n",
    "\n",
    "    def _set_data_ids(self):\n",
    "        feature_flow = self.dataflows[self.task][self.mode]['inputs']\n",
    "        return [x.decode(\"utf-8\") for x in feature_flow[self.cell_id_name]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_ids)\n",
    "\n",
    "    def __getitem__(self, item: int) -> Dict:\n",
    "        cell_id = self.data_ids[item]\n",
    "        features = self.dataflows[self.task][self.mode]['inputs']\n",
    "        meta_data = {self.index_name: cell_id}  # , self.pos_name: features[self.col_name][item].decode(\"utf-8\")\n",
    "        meta_data = self._transform_metalabels(meta_data, cell_id)\n",
    "\n",
    "        x = features[self.features_name][item]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        meta_data[self.file_types[0]] = x\n",
    "\n",
    "        if self.dataflows[self.task][self.mode].get('targets'):\n",
    "            targets = self.dataflows[self.task][self.mode]['targets']\n",
    "            # meta_data[self.target_name] = targets[self.cell_id_name][item].decode(\"utf-8\")\n",
    "            y = targets[self.features_name][item]\n",
    "\n",
    "            if self.target_transform:\n",
    "                y = self.target_transform(y)\n",
    "\n",
    "            meta_data[self.file_types[1]] = y\n",
    "\n",
    "            return meta_data\n",
    "        else:\n",
    "            return meta_data\n",
    "\n",
    "    def get_hdf5_flow(self, file_path: str):\n",
    "        file_flow = h5py.File(file_path, 'r')\n",
    "\n",
    "        file_keys = list(file_flow.keys())\n",
    "        assert len(file_keys) == 1, AssertionError(f\"Incorrect file format, '{file_path}' file have more than one \"\n",
    "                                                   f\"group: {file_keys}.\")\n",
    "\n",
    "        file_name = file_keys[0]\n",
    "        assert file_name in self.h5_reserved_names, \\\n",
    "            AssertionError(f\"Incorrect file format, group name must be in {self.h5_reserved_names}, \"\n",
    "                           f\"but {file_name} was found.\")\n",
    "\n",
    "        datasets_names = list(file_flow[file_name])\n",
    "        assert self.features_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name \"\n",
    "                                                                    f\"{self.features_name} was not found in hdf5 file \"\n",
    "                                                                    f\"datasets list.\")\n",
    "        assert self.cell_id_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name \"\n",
    "                                                                   f\"{self.cell_id_name} was not found in hdf5 file \"\n",
    "                                                                   f\"datasets list.\")\n",
    "        assert self.col_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name {self.col_name} \"\n",
    "                                                               f\"was not found in hdf5 file datasets list.\")\n",
    "\n",
    "        lines, features_shape = file_flow[file_name][self.features_name].shape\n",
    "\n",
    "        return file_flow[file_name], (lines, features_shape)\n",
    "\n",
    "    def reindex_dataset(self,\n",
    "                        day: Optional[Union[int, List[int]]] = None,\n",
    "                        donor: Optional[Union[int, List[int]]] = None,\n",
    "                        cell_type: Optional[Union[str, List[str]]] = None) -> None:\n",
    "        conditions = []\n",
    "        if (day is not None) and isinstance(day, int):\n",
    "            conditions.append((self.metadata['day'] == day))\n",
    "        elif (day is not None) and isinstance(day, list):\n",
    "            conditions.append((self.metadata['day'].isin(day)))\n",
    "\n",
    "        if (donor is not None) and isinstance(donor, int):\n",
    "            conditions.append((self.metadata['donor'] == donor))\n",
    "        elif (donor is not None) and isinstance(donor, list):\n",
    "            conditions.append((self.metadata['donor'].isin(donor)))\n",
    "\n",
    "        if (cell_type is not None) and isinstance(cell_type, int):\n",
    "            conditions.append((self.metadata['cell_type'] == cell_type))\n",
    "        elif (cell_type is not None) and isinstance(cell_type, list):\n",
    "            conditions.append((self.metadata['cell_type'].isin(cell_type)))\n",
    "\n",
    "        if len(conditions) > 0:\n",
    "            feature_flow = self.dataflows[self.task][self.mode]['inputs']\n",
    "            ids = {x.decode(\"utf-8\") for x in feature_flow[self.cell_id_name]}\n",
    "\n",
    "            final_cond = conditions[0]\n",
    "            if len(conditions) > 1:\n",
    "                for cond in conditions[1:]:\n",
    "                    final_cond &= cond\n",
    "\n",
    "            cond_index = set(self.metadata[final_cond].index)\n",
    "            self.data_ids = list(cond_index & ids)\n",
    "\n",
    "    def rebase(self, task: Optional[str] = None, mode: Optional[str] = None):\n",
    "        if task is not None:\n",
    "            self.task = task\n",
    "        if mode is not None:\n",
    "            self.mode = mode\n",
    "\n",
    "        self._read_task_dataset(self.dataset_path)\n",
    "        self.data_ids = self._set_data_ids()\n",
    "\n",
    "    def set_length(self, length: int) -> None:\n",
    "        self.data_ids = self.data_ids[:length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0172de52-b34b-4c4d-a19d-59ae77b9de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine dataset folder and get column names\n",
    "dataset_path = Path('/home/mks/PycharmProjects/multimodal_single_cell_integration/dataset')\n",
    "submissions_path = Path('/home/mks/PycharmProjects/multimodal_single_cell_integration/submissions/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cb7cc-befa-4ad7-9230-8b859033d3d5",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53501f0f-92d8-49db-a139-1c94f4dd24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2img(x: np.array) -> np.array:\n",
    "    l = np.sqrt(x.shape[0])\n",
    "    if l % int(l) != 0:\n",
    "        z = int(l)\n",
    "        new_shape = (z, z + 1)\n",
    "        new_x = np.pad(x, (0, ((z * (z + 1)) - x.shape[0])))\n",
    "        new_x = np.reshape(new_x, (z, z + 1))\n",
    "    else:\n",
    "        z = int(l)\n",
    "        new_x = np.reshape(x, (z, z))\n",
    "    \n",
    "    new_x = new_x / np.max(new_x)\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "\n",
    "def t2img(x: np.array) -> np.array:\n",
    "    l = np.sqrt(x.shape[0])\n",
    "    if l % int(l) != 0:\n",
    "        z = int(l)\n",
    "        new_shape = (z + 1, z + 1)\n",
    "        new_x = np.pad(x, (0, ((z + 1)**2 - x.shape[0])))\n",
    "        new_x = np.reshape(new_x, (z + 1, z + 1))\n",
    "    else:\n",
    "        z = int(l)\n",
    "        new_x = np.reshape(x, (z, z))\n",
    "    \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ff415e-3474-486d-abfb-007bb58d667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Reading train_multi_inputs.h5 file ... ]\n",
      "[ Reading train_multi_inputs.h5 file is complete. ]\n",
      "[ Reading train_multi_targets.h5 file ... ]\n",
      "[ Reading train_multi_targets.h5 file is complete. ]\n"
     ]
    }
   ],
   "source": [
    "dataset = FSCCDataset(dataset_path, 'multi', 'train', transform=f2img, target_transform=t2img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bfd4193-e258-4aa8-a646-0218de91e3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 479)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dataset[0]['inputs']\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf9cb42c-90b1-4a47-9520-c1c087620865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 154)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset[0]['targets']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3094a1-5863-409c-8332-aa9c2517c186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "154 - 148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92ac8414-3932-485e-b88d-93d5a05628ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8923bfa-44ac-478f-9c51-11a5471d8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray((y/np.max(y)) * 255)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec39fb1-4d3d-48cd-b3dd-c9f929827f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
