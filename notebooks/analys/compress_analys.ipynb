{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5f76a1-8724-4fc5-a53e-4c147825e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "# не удалять! import hdf5plugin !\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adfcd8b-691e-4c4d-802d-9532fb26cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSCCDataset(Dataset):\n",
    "    file_types = ['inputs', 'targets']\n",
    "    h5_reserved_names: List[str] = ['train_multi_inputs', 'train_multi_targets', 'train_cite_inputs',\n",
    "                                    'train_cite_targets', 'test_multi_inputs', 'test_cite_inputs']\n",
    "\n",
    "    dataflows = {'cite': {'train': {'inputs': None, 'targets': None},\n",
    "                          'test': {'inputs': None}},\n",
    "                 'multi': {'train': {'inputs': None, 'targets': None},\n",
    "                           'test': {'inputs': None}}}\n",
    "\n",
    "    metadata = None\n",
    "    meta_unique_vals: Dict = {}\n",
    "    metadata_file: str = 'metadata.csv'\n",
    "    meta_transform_names: List[str] = ['day', 'donor', 'cell_type']\n",
    "    meta_names: List[str] = ['day', 'donor', 'cell_type', 'technology']\n",
    "    meta_keys: List[str] = ['cell_id', 'day', 'donor', 'cell_type', 'technology']\n",
    "\n",
    "    col_name: str = 'axis0'\n",
    "    pos_name: str = 'position'\n",
    "    index_name: str = 'cell_id'\n",
    "    cell_id_name: str = \"axis1\"\n",
    "    target_name: str = 'gene_id'\n",
    "    features_name: str = \"block0_values\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_path: Union[str, Path],\n",
    "                 task: str, mode: str,\n",
    "                 meta_transform: Optional[str] = None,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None):\n",
    "        self.task = task\n",
    "        self.mode = mode\n",
    "        self.data_ids = None\n",
    "        self.data_shapes = None\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.meta_transform = meta_transform\n",
    "        # init dataset\n",
    "        self._read_task_dataset(dataset_path)\n",
    "\n",
    "    def _read_metadata(self, path: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path, index_col=self.index_name)\n",
    "        for key in self.meta_names:\n",
    "            self.meta_unique_vals[key] = list(df[key].unique())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _transform_metalabels(self, meta_dict: Dict, cell_id: str) -> Dict:\n",
    "        if self.meta_transform:\n",
    "            if self.meta_transform == 'index':\n",
    "                for key in self.meta_transform_names:\n",
    "                    meta_dict[key] = self.meta_unique_vals[key].index(self.metadata[key][cell_id])\n",
    "            elif self.meta_transform == 'one_hot':\n",
    "                for key in self.meta_transform_names:\n",
    "                    one_hot_vector = np.zeros((len(self.meta_unique_vals[key]),))\n",
    "                    one_hot_vector[self.meta_unique_vals[key].index(self.metadata[key][cell_id])] = 1\n",
    "                    meta_dict[key] = one_hot_vector\n",
    "            else:\n",
    "                raise ValueError(f\"The argument 'meta_transform' can only take values from a list \"\n",
    "                                 f\"['index', 'one_hot', None], but '{self.meta_transform}' was found.\")\n",
    "        else:\n",
    "            meta_dict = {key: self.metadata[key][cell_id] for key in self.meta_names}\n",
    "\n",
    "        return meta_dict\n",
    "\n",
    "    def _get_task_flow(self, folder_path: Path, mode: str, task: str, file_type: str) -> None:\n",
    "        file_name = '_'.join([mode, task, file_type])\n",
    "        print(f\"[ Reading {file_name}.h5 file ... ]\")\n",
    "        f_path = str(folder_path.joinpath(f\"{file_name}.h5\").absolute())\n",
    "        flow, feature_shape = self.get_hdf5_flow(f_path)\n",
    "        # write data in structure\n",
    "        self.dataflows[task][mode][file_type] = flow\n",
    "        self.data_shapes[task][mode][file_type] = feature_shape\n",
    "        print(f\"[ Reading {file_name}.h5 file is complete. ]\")\n",
    "\n",
    "    def _read_task_dataset(self, folder_path: Union[str, Path]) -> None:\n",
    "        self.data_shapes = {self.task: {self.mode: {s: None for s in self.file_types}}}\n",
    "\n",
    "        if isinstance(folder_path, str):\n",
    "            folder_path = Path(folder_path)\n",
    "        # read metadata file\n",
    "        self.metadata = self._read_metadata(str(folder_path.joinpath(self.metadata_file)))\n",
    "        # read all h5 files\n",
    "        if self.mode == 'train':\n",
    "            for file_type in self.file_types:\n",
    "                self._get_task_flow(folder_path, self.mode, self.task, file_type)\n",
    "        elif self.mode == 'test':\n",
    "            self._get_task_flow(folder_path, self.mode, self.task, self.file_types[0])\n",
    "        else:\n",
    "            raise ValueError(f\"Argument 'mode' can only take values from a list: ['train', 'test'], \"\n",
    "                             f\"but {self.mode} was found.\")\n",
    "\n",
    "        self.data_ids = self._set_data_ids()\n",
    "\n",
    "    def _set_data_ids(self):\n",
    "        feature_flow = self.dataflows[self.task][self.mode]['inputs']\n",
    "        return [x.decode(\"utf-8\") for x in feature_flow[self.cell_id_name]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_ids)\n",
    "\n",
    "    def __getitem__(self, item: int) -> Dict:\n",
    "        cell_id = self.data_ids[item]\n",
    "        features = self.dataflows[self.task][self.mode]['inputs']\n",
    "        meta_data = {self.index_name: cell_id}  # self.pos_name: features[self.col_name][item].decode(\"utf-8\")\n",
    "        meta_data = self._transform_metalabels(meta_data, cell_id)\n",
    "\n",
    "        x = features[self.features_name][item]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        meta_data[self.file_types[0]] = x\n",
    "\n",
    "        if self.dataflows[self.task][self.mode].get('targets'):\n",
    "            targets = self.dataflows[self.task][self.mode]['targets']\n",
    "            # meta_data[self.target_name] = targets[self.cell_id_name][item].decode(\"utf-8\")\n",
    "            y = targets[self.features_name][item]\n",
    "\n",
    "            if self.target_transform:\n",
    "                y = self.target_transform(y)\n",
    "\n",
    "            meta_data[self.file_types[1]] = y\n",
    "\n",
    "            return meta_data\n",
    "        else:\n",
    "            return meta_data\n",
    "\n",
    "    def get_hdf5_flow(self, file_path: str):\n",
    "        file_flow = h5py.File(file_path, 'r')\n",
    "\n",
    "        file_keys = list(file_flow.keys())\n",
    "        assert len(file_keys) == 1, AssertionError(f\"Incorrect file format, '{file_path}' file have more than one \"\n",
    "                                                   f\"group: {file_keys}.\")\n",
    "\n",
    "        file_name = file_keys[0]\n",
    "        assert file_name in self.h5_reserved_names, \\\n",
    "            AssertionError(f\"Incorrect file format, group name must be in {self.h5_reserved_names}, \"\n",
    "                           f\"but {file_name} was found.\")\n",
    "\n",
    "        datasets_names = list(file_flow[file_name])\n",
    "        assert self.features_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name \"\n",
    "                                                                    f\"{self.features_name} was not found in hdf5 file \"\n",
    "                                                                    f\"datasets list.\")\n",
    "        assert self.cell_id_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name \"\n",
    "                                                                   f\"{self.cell_id_name} was not found in hdf5 file \"\n",
    "                                                                   f\"datasets list.\")\n",
    "        assert self.col_name in datasets_names, AssertionError(f\"Incorrect file format, dataset name {self.col_name} \"\n",
    "                                                               f\"was not found in hdf5 file datasets list.\")\n",
    "\n",
    "        lines, features_shape = file_flow[file_name][self.features_name].shape\n",
    "\n",
    "        return file_flow[file_name], (lines, features_shape)\n",
    "\n",
    "    def reindex_dataset(self,\n",
    "                        day: Optional[Union[int, List[int]]] = None,\n",
    "                        donor: Optional[Union[int, List[int]]] = None,\n",
    "                        cell_type: Optional[Union[str, List[str]]] = None) -> None:\n",
    "        conditions = []\n",
    "        if (day is not None) and isinstance(day, int):\n",
    "            conditions.append((self.metadata['day'] == day))\n",
    "        elif (day is not None) and isinstance(day, list):\n",
    "            conditions.append((self.metadata['day'].isin(day)))\n",
    "\n",
    "        if (donor is not None) and isinstance(donor, int):\n",
    "            conditions.append((self.metadata['donor'] == donor))\n",
    "        elif (donor is not None) and isinstance(donor, list):\n",
    "            conditions.append((self.metadata['donor'].isin(donor)))\n",
    "\n",
    "        if (cell_type is not None) and isinstance(cell_type, int):\n",
    "            conditions.append((self.metadata['cell_type'] == cell_type))\n",
    "        elif (cell_type is not None) and isinstance(cell_type, list):\n",
    "            conditions.append((self.metadata['cell_type'].isin(cell_type)))\n",
    "\n",
    "        if len(conditions) > 0:\n",
    "            feature_flow = self.dataflows[self.task][self.mode]['inputs']\n",
    "            ids = {x.decode(\"utf-8\") for x in feature_flow[self.cell_id_name]}\n",
    "\n",
    "            final_cond = conditions[0]\n",
    "            if len(conditions) > 1:\n",
    "                for cond in conditions[1:]:\n",
    "                    final_cond &= cond\n",
    "\n",
    "            cond_index = set(self.metadata[final_cond].index)\n",
    "            self.data_ids = list(cond_index & ids)\n",
    "\n",
    "    def rebase(self, task: Optional[str] = None, mode: Optional[str] = None):\n",
    "        if task is not None:\n",
    "            self.task = task\n",
    "        if mode is not None:\n",
    "            self.mode = mode\n",
    "\n",
    "        self._read_task_dataset(self.dataset_path)\n",
    "        self.data_ids = self._set_data_ids()\n",
    "\n",
    "    def set_length(self, length: int) -> None:\n",
    "        self.data_ids = self.data_ids[:length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb734768-b090-4a9e-bccc-4bf428fdb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '/home/mks/PycharmProjects/multimodal_single_cell_integration/dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a6a01-9295-4ab4-ae7d-e3865480f118",
   "metadata": {},
   "source": [
    "# Считаем количество ненулевых элементов в ATAC фичах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a82fbcd-48cc-4dc7-95d0-10ed48105bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Reading train_multi_inputs.h5 file ... ]\n",
      "[ Reading train_multi_inputs.h5 file is complete. ]\n",
      "[ Reading train_multi_targets.h5 file ... ]\n",
      "[ Reading train_multi_targets.h5 file is complete. ]\n",
      "[ Reading test_multi_inputs.h5 file ... ]\n",
      "[ Reading test_multi_inputs.h5 file is complete. ]\n"
     ]
    }
   ],
   "source": [
    "mtrain = FSCCDataset(dataset_folder, 'multi', 'train')\n",
    "mtest = FSCCDataset(dataset_folder, 'multi', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e39d3d9-8a22-4c23-89f5-275095e40c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: {'day': [3, 4, 7, 2, 10], 'donor': [27678, 32606, 13176, 31800], 'cell_type': ['MasP', 'MkP', 'NeuP', 'HSC', 'EryP', 'MoP', 'BP', 'hidden'], 'technology': ['citeseq', 'multiome']}\n",
      "\n",
      "Dataset 1: {'day': [3, 4, 7, 2, 10], 'donor': [27678, 32606, 13176, 31800], 'cell_type': ['MasP', 'MkP', 'NeuP', 'HSC', 'EryP', 'MoP', 'BP', 'hidden'], 'technology': ['citeseq', 'multiome']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate([mtrain, mtest]):\n",
    "    print(f'Dataset {i}: {d.meta_unique_vals}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e76c03-d994-4d73-a16f-303aa0ee11f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: {'multi': {'train': {'inputs': (105942, 228942), 'targets': (105942, 23418)}}}\n",
      "\n",
      "Dataset 1: {'multi': {'test': {'inputs': (55935, 228942), 'targets': None}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate([mtrain, mtest]):\n",
    "    print(f'Dataset {i}: {d.data_shapes}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab12e981-c58d-42e3-8dc5-968fa963c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105942/105942 [03:26<00:00, 512.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55935/55935 [01:08<00:00, 822.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимум: 35837\n",
      "Минимум: 746\n",
      "Среднее: 5935.829401335582\n",
      "Стандартное отклонение: 4122.413955322037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_non_zero = -np.inf\n",
    "min_non_zero = np.inf\n",
    "non_zero_array = []\n",
    "for d, mode in zip([mtrain, mtest], ['train', 'test']):\n",
    "    for ind in tqdm(range(len(d))):\n",
    "        nz_count = np.count_nonzero(d[ind]['inputs'])\n",
    "        if nz_count > max_non_zero:\n",
    "            max_non_zero = nz_count\n",
    "        \n",
    "        if nz_count < min_non_zero:\n",
    "            min_non_zero = nz_count\n",
    "        \n",
    "        non_zero_array.append(nz_count)\n",
    "\n",
    "print(f\"Максимум: {max_non_zero}\")\n",
    "print(f\"Минимум: {min_non_zero}\")\n",
    "print(f\"Среднее: {np.mean(non_zero_array)}\")\n",
    "print(f\"Стандартное отклонение: {np.std(non_zero_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e4dadd-606b-4232-8f1f-bf0f4a559976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15653309571856627"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35837 / 228942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78d3453-4b41-46d2-be28-c28cf9a1d58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032584672100357294"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "746 / 228942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e7e9a9f-0a4b-4e2d-9835-9fbfb4ffe324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645066"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35837 * len(np.base_repr(228942, base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef622f9f-6f9e-48fd-b719-933e2007729a",
   "metadata": {},
   "source": [
    "# Считаем количество ненулевых элементов в GeneExp фичах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c742f0-2824-431a-b387-52d1e17af55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Reading train_cite_inputs.h5 file ... ]\n",
      "[ Reading train_cite_inputs.h5 file is complete. ]\n",
      "[ Reading train_cite_targets.h5 file ... ]\n",
      "[ Reading train_cite_targets.h5 file is complete. ]\n",
      "[ Reading test_cite_inputs.h5 file ... ]\n",
      "[ Reading test_cite_inputs.h5 file is complete. ]\n"
     ]
    }
   ],
   "source": [
    "ctrain = FSCCDataset(dataset_folder, 'cite', 'train')\n",
    "ctest = FSCCDataset(dataset_folder, 'cite', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a40e8d1-beb9-4513-b6d1-3ceedb35f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: {'day': [3, 4, 7, 2, 10], 'donor': [27678, 32606, 13176, 31800], 'cell_type': ['MasP', 'MkP', 'NeuP', 'HSC', 'EryP', 'MoP', 'BP', 'hidden'], 'technology': ['citeseq', 'multiome']}\n",
      "\n",
      "Dataset 1: {'day': [3, 4, 7, 2, 10], 'donor': [27678, 32606, 13176, 31800], 'cell_type': ['MasP', 'MkP', 'NeuP', 'HSC', 'EryP', 'MoP', 'BP', 'hidden'], 'technology': ['citeseq', 'multiome']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate([ctrain, ctest]):\n",
    "    print(f'Dataset {i}: {d.meta_unique_vals}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf685d2-cafc-4199-b98a-a137b563df3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: {'cite': {'train': {'inputs': (70988, 22050), 'targets': (70988, 140)}}}\n",
      "\n",
      "Dataset 1: {'cite': {'test': {'inputs': (48203, 22050), 'targets': None}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate([ctrain, ctest]):\n",
    "    print(f'Dataset {i}: {d.data_shapes}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a11c8ad2-4035-4a0f-a496-445e712adb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70988/70988 [01:16<00:00, 928.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48203/48203 [01:20<00:00, 596.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимум: 8581\n",
      "Минимум: 1682\n",
      "Среднее: 4835.0436610146735\n",
      "Стандартное отклонение: 1049.0076257383432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_non_zero = -np.inf\n",
    "min_non_zero = np.inf\n",
    "non_zero_array = []\n",
    "for d, mode in zip([ctrain, ctest], ['train', 'test']):\n",
    "    for ind in tqdm(range(len(d))):\n",
    "        nz_count = np.count_nonzero(d[ind]['inputs'])\n",
    "        if nz_count > max_non_zero:\n",
    "            max_non_zero = nz_count\n",
    "        \n",
    "        if nz_count < min_non_zero:\n",
    "            min_non_zero = nz_count\n",
    "        \n",
    "        non_zero_array.append(nz_count)\n",
    "\n",
    "print(f\"Максимум: {max_non_zero}\")\n",
    "print(f\"Минимум: {min_non_zero}\")\n",
    "print(f\"Среднее: {np.mean(non_zero_array)}\")\n",
    "print(f\"Стандартное отклонение: {np.std(non_zero_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f133c3-b0f9-4521-9d5c-eac8a1347c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3891609977324263"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8581 / 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15b5e85-f0b5-479f-ae1a-483514910494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076281179138322"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1682 / 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6323e7c0-b64a-4600-8f09-6c694d42d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21695966907962771"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1049 / 4835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bfbd073-62e0-43d3-bc73-845a03bbf49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128715"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8581 * len(np.base_repr(22050, base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ef97d-f694-4e41-a01e-8d256b99fb73",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "177306c4-6b5b-40c1-8f2b-96c540dd5991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrain[10]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c8304-fca4-49e7-b27b-12de09017de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
